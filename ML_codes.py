# -*- coding: utf-8 -*-
"""18152.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l51wyXNtgdZnVd759vMP45gpk2xegMHu

## **Importing Libraries**
"""

###Importing Necessary Libraries
import pandas as pd
import csv
import numpy as np
import matplotlib.pyplot as plt
import math

import warnings
warnings.filterwarnings("ignore")

import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import pairwise_distances
import sklearn.metrics

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import roc_curve, auc
from sklearn.datasets import load_digits
from sklearn.model_selection import cross_val_score

from sklearn.mixture import GaussianMixture
from sklearn import mixture

from sklearn.cluster import KMeans

from sklearn.neighbors import KNeighborsClassifier

from sklearn import datasets, linear_model, metrics

from sklearn.linear_model import LogisticRegression

from sklearn import svm
from sklearn.svm import SVC

from sklearn import tree

from sklearn.ensemble import RandomForestClassifier

from sklearn.neural_network import MLPClassifier

"""## **Processing Total Data**"""

###Reading CSV file for Total Data
total_set_dummy = pd.read_csv("Total_stars.csv")
#print(total_set_dummy.columns.values)
del total_set_dummy['_r']

#total_set_reduced = total_set_dummy[['A','D']]
total_set_1 = total_set_dummy.dropna()

###Adding a column for Distance from Sun
total_set_1['dist'] = 1000/total_set_1['Plx']

###Applying Constraints to the Enhance Data
cleaned_set = total_set_1[(total_set_1.Plx > 0) & (abs(total_set_1.pmRA) <= 20) & (abs(total_set_1.pmDE) <= 20) & (total_set_1.dist <= 1600) & (total_set_1.dist >= 500)]
total_set = cleaned_set[['_RAJ2000','_DEJ2000', 'Plx', 'pmRA', 'pmDE']]
total_set_KMC = cleaned_set[['_RAJ2000','_DEJ2000', 'Plx', 'pmRA', 'pmDE']]
#print(total_set.columns.values)

'''
###Plotting the Cleaned Data
plt.scatter(total_set["BP-RP"], total_set["Gmag"], c='blue', s = 1, label = "Cleaned Data")
plt.ylim(22,8)
plt.show()
'''

"""# **UNSUPERVISED LEARNING**

## **Gaussian Mixture Model**
"""

###Reading CSV file for Total Data
total_set_dummy = pd.read_csv("Total_stars.csv")
#print(total_set_dummy.columns.values)
del total_set_dummy['_r']

#total_set_reduced = total_set_dummy[['A','D']]
total_set_1 = total_set_dummy.dropna()

###Adding a column for Distance from Sun
total_set_1['dist'] = 1000/total_set_1['Plx']

###Applying Constraints to the Enhance Data
cleaned_set = total_set_1[(total_set_1.Plx > 0) & (abs(total_set_1.pmRA) <= 20) & (abs(total_set_1.pmDE) <= 20) & (total_set_1.dist <= 1600) & (total_set_1.dist >= 500)]
total_set = cleaned_set[['_RAJ2000','_DEJ2000', 'Plx', 'pmRA', 'pmDE']]
total_set_KMC = cleaned_set[['_RAJ2000','_DEJ2000', 'Plx', 'pmRA', 'pmDE']]
#print(total_set.columns.values)

#gmm = GaussianMixture(n_components=2)
#gmm.fit(total_set)

gmm = GaussianMixture(n_components=2, 
                      covariance_type='diag',    #"full", "spherical", "tied", "diag"
                      tol=0.001,                 #"0.01", "0.0001", "0.001"
                      reg_covar=1*math.exp(-6),  #"e**(-4)", "e**(-8)", "e**(-6)"
                      max_iter=100,              #50, 100, 200, 500
                      n_init=100,                  #0, 1, 2
                      init_params='kmeans',      #'kmeans', 'random'
                      weights_init=None, 
                      means_init=None, 
                      precisions_init=None, 
                      random_state=None,         #0, 1
                      warm_start=False,          #True, False
                      verbose=0,                 #0, 1
                      verbose_interval=1)       #10, 5, 20
#gmm = mixture.GMM(n_components=2, init_params='wmc', covariance_type='diag',)
gmm.fit(total_set)

#predictions from gmm
labels = gmm.predict(total_set)
prblty = gmm.predict_proba(total_set)

samp = 8

print("------------ANALYSIS RESULTS FOR GMM-------------\n")

converg = gmm.converged_
print("Convergence:",converg)

iter = gmm.n_iter_
print("Number of iterations needed for convergence",iter, "\n")

means = gmm.means_
weights = gmm.weights_

print("Values of Means:\n")
for i in range(means.shape[0]):
  for j in range(means.shape[1]):
    print("Mean for class", i, "for feature", total_set.columns.values[j], "is", means[i][j])
  print("Weight for class", i, "is", weights[i], "\n")

covar = gmm.covariances_
print("Covariance Matrix:\n", covar, "\n")

scr_samp = gmm.score_samples(total_set)
print("The weighted log probabilities:\n", scr_samp)

scr = gmm.score(total_set)
print("\nAverage Log Likelihood:", scr)

lowbo = gmm.lower_bound_
print("\nLower Bound for Log Likelihood is given by", lowbo, "\n")

silh = metrics.silhouette_score(total_set, labels, metric='euclidean')
print("Average Silhouette Score :", silh, "\n")

CH_score = metrics.calinski_harabasz_score(total_set, labels)
print("Calinski Harabasz Index:", CH_score, "\n")

DB_score = metrics.davies_bouldin_score(total_set, labels)
print("Davies Bouldin Index:", DB_score, "\n")

silh_samp = metrics.silhouette_samples(total_set, labels, metric='euclidean')

plt.figure(figsize = (18,12))
plt.suptitle('GMM Analysis details: Number of iterations for convergence: '+str(iter)+"\nWeights for classes: "+str(weights)+"  Average Log Likelihood: "+str(round(scr,4))+"\nSilhouette Score: "+str(round(silh,4))+"  Calinski Harabasz Index: "+str(round(CH_score,4))+"  Davies Bouldin Index: "+str(round(DB_score, 4)), fontsize = 18)

plt.subplot(1, 2, 1)
plt.hist(silh_samp, bins = 20)
plt.title("Silhouette Score Distribution for GMM Model", fontsize = 15)
plt.grid()

plt.subplot(1, 2, 2)
plt.hist(prblty[:, 0], bins = 20)
plt.title("Probablity Distribution for GMM Model", fontsize = 15)
#plt.ylim(0,100)
plt.grid()
plt.show()
#print(prblty)
#print(labels)
#print(len(labels))

#plt.savefig("GMM_Distributions_"+str(samp)+".png")

total_set["Gmag"] = cleaned_set["Gmag"]
total_set["BPmag"] = cleaned_set["BPmag"]
total_set["RPmag"] = cleaned_set["RPmag"]
total_set["BP-RP"] = cleaned_set["BP-RP"]
total_set["BP-G"] = cleaned_set["BP-G"]
total_set["G-RP"] = cleaned_set["G-RP"]
total_set["classes"] = labels

total_set["probablity_0"] = prblty[:,0]
total_set["probablity_1"] = prblty[:,1]
#print(total_set.columns.values)

###Making Separate Dataframe for the Output Classes
cls1 = total_set[(total_set.probablity_0 >= 0.99)]
cls0 = total_set[(total_set.probablity_1 >= 0.99)]

del total_set['probablity_0']
del total_set['probablity_1']

###Plotting the Output
color=['blue','red']
plt.figure(figsize = (18,12))

#plt.suptitle('BP-RP vs Gmag and pmRA vs pmDE Plots of Cluster Made Using GMM\nNumber of stars in class 0: '+str(len(cls0.axes[0]))+"  Number of Stars in class 1: "+str(len(cls1.axes[0]))+"\nMean for class 0 in pmRA vs pmDE plane is: "+str(round(means[0][3],4))+","+str(round(means[0][4],4))+"\nMean for class 1 in pmRA vs pmDE plane is: "+str(round(means[1][3],4))+","+str(round(means[1][4],4)), fontsize = 18)
plt.suptitle('BP-RP vs Gmag and pmRA vs pmDE Plots of Cluster Made Using GMM\nNumber of stars in class 0: '+str(len(cls0.axes[0]))+"  Number of Stars in class 1: "+str(len(cls1.axes[0]))+"\nMean for class 0 in pmRA vs pmDE plane is: "+str(round(means[1][3],4))+","+str(round(means[1][4],4))+"\nMean for class 1 in pmRA vs pmDE plane is: "+str(round(means[0][3],4))+","+str(round(means[0][4],4)), fontsize = 18)

### Drawing Graph for class 0 Stars
print("Number of Class 0 Stars =", len(cls0.axes[0]))
plt.subplot(2, 3, 1)
plt.scatter(cls0["BP-RP"], cls0["Gmag"], c=color[0], s = 1, label = "class 0")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 1 Stars
print("Number of Class 1 Stars =", len(cls1.axes[0]),"\n")
plt.subplot(2, 3, 2)
plt.scatter(cls1["BP-RP"], cls1["Gmag"], c=color[1], s = 1, label = "Class 1")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Combined Graph for classified Stars
plt.subplot(2, 3, 3)
plt.scatter(cls0["BP-RP"], cls0["Gmag"], c=color[0], s = 1, label = "class 0")
plt.scatter(cls1["BP-RP"], cls1["Gmag"], c=color[1], s = 1, label = "Class 1")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()
#plt.show()

#plt.figure(figsize = (21,7))

### Drawing Graph for class 0 Stars
plt.subplot(2, 3, 4)
plt.scatter(cls0["pmRA"], cls0["pmDE"], c=color[0], s = 1, label = "class 0")

plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 1 Stars

plt.subplot(2, 3, 5)
plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Combined Graph for classified Stars
plt.subplot(2, 3, 6)
#plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(cls0["pmRA"], cls0["pmDE"], c=color[0], s = 1, label = "Class 0")
plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()
plt.show()

#plt.savefig("GMM_"+str(samp)+".png")

"""# **Preparing Testing and Training Data**

## **Preparing Training Data**
"""

###Getting Training Data from GMM Results
frames = [cls0, cls1]
train_set = pd.concat(frames)

del train_set['probablity_0']
del train_set['probablity_1']

###Separating input data from combined Training Data
inp_val_train = train_set.iloc[:, 0:11].values 

###Separating output data from combined Training Data
out_val_train = train_set.iloc[:, 11].values 

###Normalising the Training set
stan_scaler = StandardScaler()

norm_inp_train = stan_scaler.fit_transform(inp_val_train)
#print(norm_inp_train.shape)

###Dividing Input Data into Model Training and Model Testing
xtrain, xtest, ytrain, ytest = train_test_split(norm_inp_train, out_val_train, test_size = 0.20, random_state = 0)

"""## **Preparing Testing Data**"""

###Reading CSV file for Testing Data
test_set_dummy = pd.read_csv("Total_stars.csv")
test_set = test_set_dummy.dropna()

###Removing Identity Column from Testing Data
inp_val_test = test_set.iloc[:, 1:12].values

###Normalising the Testing Set
norm_inp_test = stan_scaler.transform(inp_val_test)

#plt.scatter(test_set["BP-RP"], test_set["Gmag"], c="blue", s = 1, label = "Testing Set")
#plt.show()

len(test_set.axes[0])
train_set.columns.values

"""# **SUPERVISED LEARNING**

## **Linear Regression**
"""

BP_RP = cls0['BPmag']-cls0["RPmag"]

inp_LR = np.asarray(BP_RP).reshape(-1,1)
out_LR = cls0['Gmag']

xtrain_LR, xtest_LR, ytrain_LR, ytest_LR = train_test_split(inp_LR, out_LR, test_size=0.20,
                                                    random_state=1)

###Modelling Via Linear Regression
model_lin_reg = linear_model.LinearRegression()
model_lin_reg.fit(xtrain_LR, ytrain_LR)

# regression coefficients
print('Coefficients: ', model_lin_reg.coef_)

print("Intercept: ", model_lin_reg.intercept_)

print("Equation of line in CMD palne: Gmag = "+str(round(model_lin_reg.coef_[0],4))+".(BP-RP) + "+str(round(model_lin_reg.intercept_,4)))
 
# variance score: 1 means perfect prediction
print('R^2 prediction for the model:',round(model_lin_reg.score(xtest_LR, ytest_LR),4))

Gmag_out = model_lin_reg.predict(xtest_LR)

#BP_RP_in = np.asarray(xtest_LR)

test_BP_RP = test_set['BPmag']-test_set['RPmag']
test_Gmag = test_set["Gmag"]
test_BP_RP_in = np.asarray(test_BP_RP).reshape(-1,1)

test_Gmag_out = model_lin_reg.predict(test_BP_RP_in)

print("Variance Score:", metrics.explained_variance_score(ytest_LR, Gmag_out))

print("R-squared Value :", round(metrics.r2_score(ytest_LR, Gmag_out),4))

print("Mean Absolute Error:", metrics.mean_absolute_error(ytest_LR, Gmag_out))

print("Mean Squared error:", metrics.mean_squared_error(ytest_LR, Gmag_out))

plt.figure(figsize = (16,8))

plt.suptitle('BP-RP vs Gmag Plots of Cluster for Training and Total Cluster Data Made Using Linear Regression', fontsize = 16, fontweight='bold')

plt.subplot(1, 2, 1)

plt.scatter(BP_RP, out_LR,  s=1, label = "Cluster Stars")
plt.plot(test_BP_RP_in, test_Gmag_out, color ="red", label = "Predicted Line" )

plt.title("Coefficients = "+str(np.around(model_lin_reg.coef_,4))+
          "  Intercept = "+str(np.around(model_lin_reg.intercept_,4))+
          "\n R-squared Value = "+str(round(metrics.r2_score(ytest_LR, Gmag_out),4))+
          "  Variance = "+str(round(metrics.explained_variance_score(ytest_LR, Gmag_out),4)), fontsize = 15)

plt.xlabel("pmRA", fontsize=10)
plt.ylabel("pmDE", fontsize=10)
plt.legend()
plt.ylim(24,7)
plt.grid()

plt.subplot(1, 2, 2)

plt.plot(test_BP_RP_in, test_Gmag_out, color = "red", label = "Predicted Line" )
plt.scatter(test_BP_RP, test_Gmag , s=1, label = "Total Stars")

plt.title("Mean Squared Error = "+str(np.around(metrics.mean_squared_error(ytest_LR, Gmag_out),4))+
          "\nMean Absolute Error = "+str(np.around(metrics.mean_absolute_error(ytest_LR, Gmag_out),4))
          , fontsize = 15)

plt.xlabel("BP-RP", fontsize=10)
plt.ylabel("Gmag", fontsize=10)
plt.legend()
plt.ylim(24,7)
plt.grid()
plt.show()

#plt.savefig("Lin_Reg_cmd")

###DOING SAME FOR pmRA vs pmDE plane
pmRA = cls0['pmRA']

pmRA_inp_LR = np.asarray(pmRA).reshape(-1,1)
pmDE_out_LR = cls0['pmDE']

xtrain_LR1, xtest_LR1, ytrain_LR1, ytest_LR1 = train_test_split(pmRA_inp_LR, pmDE_out_LR, test_size=0.20,
                                                    random_state=1)

###Modelling Via Linear Regression
model_lin_reg1 = linear_model.LinearRegression()
model_lin_reg1.fit(xtrain_LR1, ytrain_LR1)

# regression coefficients
print('Coefficients: ', model_lin_reg1.coef_)

print("Intercept: ", model_lin_reg1.intercept_)

print("Equation of line in CMD palne: Gmag = "+str(round(model_lin_reg1.coef_[0],4))+".(BP-RP) + "+str(round(model_lin_reg1.intercept_,4)))
 
# variance score: 1 means perfect prediction
print('R^2 prediction for the model:',round(model_lin_reg1.score(xtest_LR1, ytest_LR1),4))

pmDE_out1 = model_lin_reg1.predict(xtest_LR1)


test_pmRA1 = test_set['pmRA']
test_pmDE1 = test_set["pmDE"]
test_pmRA_in1 = np.asarray(test_pmRA1).reshape(-1,1)

test_pmDE_out1 = model_lin_reg1.predict(test_pmRA_in1)

print("Variance Score:", metrics.explained_variance_score(ytest_LR1, pmDE_out1))

print("R-squared Value :", round(metrics.r2_score(ytest_LR1, pmDE_out1),4))

print("Mean Absolute Error:", metrics.mean_absolute_error(ytest_LR1, pmDE_out1))

print("Mean Squared error:", metrics.mean_squared_error(ytest_LR1, pmDE_out1))

plt.figure(figsize = (16,8))

plt.suptitle('pmRA vs pmDE plots of Cluster for Training and Total Cluster Data Made Using Linear Regression', fontsize = 16, fontweight='bold')

plt.subplot(1, 2, 1)

plt.scatter(pmRA, pmDE_out_LR,  s=1)
plt.plot(test_pmRA_in1, test_pmDE_out1, color = "red", label = "Predicted Line" )

plt.title("Coefficients = "+str(np.around(model_lin_reg1.coef_,4))+
          "  Intercept = "+str(np.around(model_lin_reg1.intercept_,4))+
          "\n R-squared Value = "+str(round(metrics.r2_score(ytest_LR1, pmDE_out1),4))+
          "  Variance = "+str(round(metrics.explained_variance_score(ytest_LR1, pmDE_out1),4)), fontsize = 15)

plt.xlabel("pmRA", fontsize=10)
plt.ylabel("pmDE", fontsize=10)
plt.xlim(-13,-10)
plt.legend()

plt.grid()

plt.subplot(1, 2, 2)

plt.plot(test_pmRA_in1, test_pmDE_out1, color = "red", label = "Predicted Line" )
plt.scatter(test_pmRA1, test_pmDE1 , s=1, label = "Total Stars")

plt.title("Mean Squared Error = "+str(np.around(metrics.mean_squared_error(ytest_LR1, pmDE_out1),4))+
          "\nMean Absolute Error = "+str(np.around(metrics.mean_absolute_error(ytest_LR1, pmDE_out1),4))
          , fontsize = 15)

plt.xlabel("pmRA", fontsize=10)
plt.ylabel("pmDE", fontsize=10)
plt.xlim(-150, 150)
plt.legend()
plt.grid()
plt.show()
#plt.savefig("Lin_Reg_pmra")

"""## **Logistic Regression**"""

###MODELLING VIA LOGISTIC REGRESSION
model_Logi_Reg = LogisticRegression(penalty='l2', 
                                    dual=False, 
                                    tol=0.0001, 
                                    C=1.0, 
                                    fit_intercept=True, 
                                    intercept_scaling=1, 
                                    class_weight=None, 
                                    random_state=None, 
                                    solver='lbfgs', 
                                    max_iter=100, 
                                    multi_class='multinomial', 
                                    verbose=10, 
                                    warm_start=False, 
                                    n_jobs=None, 
                                    l1_ratio=None)
model_Logi_Reg.fit(xtrain, ytrain)
                   
mod = 11

class_LR = model_Logi_Reg.classes_

converg = model_Logi_Reg.n_iter_
print("\nIterations for convergence:", converg)

bias = model_Logi_Reg.intercept_
'''
bias1 = model_Logi_Reg_1.intercept_
bias2 = model_Logi_Reg_2.intercept_
'''
print("Bias added to model", bias)
'''
print("Bias added to model", bias1)
print("Bias added to model", bias2)
'''

##Output for testing of Model
predicted_class_Logi_Reg = model_Logi_Reg.predict(xtest)
'''
predicted_class_Logi_Reg_1 = model_Logi_Reg_1.predict(xtest)
predicted_class_Logi_Reg_2 = model_Logi_Reg_2.predict(xtest)
'''

##Checking Model Accuracy
accuracy_Logi_Reg = accuracy_score(ytest,predicted_class_Logi_Reg)
print("The Accuracy of Logistic Regression Model is "+str(round(accuracy_Logi_Reg*100,2))+"%.\n")

bal_acc_Logi_Reg = metrics.balanced_accuracy_score(ytest, predicted_class_Logi_Reg)
print("The Balanced Accuracy of Logistic Regression Model is "+str(round(bal_acc_Logi_Reg,2))+"%.\n")

f1_Logi_Reg = metrics.f1_score(ytest, predicted_class_Logi_Reg)
print("The f1 Score of Logistic Regression Model is "+str(round(f1_Logi_Reg,2))+".\n")

log_loss_Logi_Reg = metrics.log_loss(ytest, predicted_class_Logi_Reg)
print("The Log Loss of Logistic Regression Model is "+str(round(log_loss_Logi_Reg,2))+".\n")

recall_Logi_Reg = metrics.recall_score(ytest, predicted_class_Logi_Reg)
print("The Recall Score of Logistic Regression Model is "+str(round(recall_Logi_Reg,2))+".\n")

prec_Logi_Reg = metrics.precision_score(ytest, predicted_class_Logi_Reg)
print("The Precision Score of Logistic Regression Model is "+str(round(prec_Logi_Reg,2))+".\n")

AUC_Logi_Reg = metrics.roc_auc_score(ytest, predicted_class_Logi_Reg)
print("The Value of AUC for Logistic Regression Model is "+str(round(AUC_Logi_Reg,2))+".\n")

# Classification report
result1_LR = classification_report(ytest, predicted_class_Logi_Reg)
print("\nClassification Report:")
print (result1_LR)

conf_mat_LR = confusion_matrix(ytest, predicted_class_Logi_Reg)
print("Confusion Matrix:")
print (conf_mat_LR, "\n")

##This gives Model Parameters
parameters = model_Logi_Reg.coef_
for i in range(len(train_set.columns.values)-1):
  print("Feature coefficients of", total_set.columns.values[i], "is given by :", parameters[0][i])

yscore_LR = model_Logi_Reg.decision_function(xtest)
#yscore_LR_1 = model_Logi_Reg_1.decision_function(xtest)
#yscore_LR_2 = model_Logi_Reg_2.decision_function(xtest)


plt.figure(figsize = (6,12))
plt.suptitle('ROC and Precision vs Recall Plots\n for different Multi-Class values', fontsize = 16, fontweight='bold')

#disp = metrics.plot_roc_curve(model_Logi_Reg, xtest, ytest) 
plt.subplot(2, 1, 1)
fpr_LR, tpr_LR, _ = metrics.roc_curve(ytest, yscore_LR)
#fpr_LR_1, tpr_LR_1, _ = metrics.roc_curve(ytest, yscore_LR_1)
#fpr_LR_2, tpr_LR_2, _ = metrics.roc_curve(ytest, yscore_LR_2)


plt.plot(fpr_LR, tpr_LR, label = "ROC Curve for auto")
#plt.plot(fpr_LR_1, tpr_LR_1, label = "ROC Curve for ovr")
#plt.plot(fpr_LR_2, tpr_LR_2, label = "ROC Curve for multinomial")


plt.plot([0,1], [0,1], label = "FPR = TPR Line", color = "black")

#plt.title("AUC for ROC Curve: "+str(AUC_Logi_Reg)+"\nPrecision Score: "+str(prec_Logi_Reg)+"\nRecall Score: "+str(recall_Logi_Reg), fontsize = 14)
plt.xlabel("False Positive Rate",fontsize=14)
plt.ylabel("True Positive Rate",fontsize=14)
plt.legend()
plt.grid()
plt.show()

#disp2 =metrics.plot_precision_recall_curve(model_Logi_Reg, xtest, ytest)
plt.subplot(2, 1, 2)
precision_LR, recall_LR, _ = metrics.precision_recall_curve(ytest, yscore_LR)
#precision_LR_1, recall_LR_1, _ = metrics.precision_recall_curve(ytest, yscore_LR_1)
#precision_LR_2, recall_LR_2, _ = metrics.precision_recall_curve(ytest, yscore_LR_2)

plt.plot(recall_LR, precision_LR, label = "Recall-Precision Curve for auto")
#plt.plot(recall_LR_1, precision_LR_1, label = "Recall-Precision Curve for ovr")
#plt.plot(recall_LR_2, precision_LR_2, label = "Recall-Precision Curve for multinomial")


plt.xlabel("Recall",fontsize=14)
plt.ylabel("Precision",fontsize=14)
plt.legend()
plt.grid()
plt.show()

#plt.savefig("LogReg_curve_Multi-class_values"+str(mod)+".png")
###Putting Logistic Regression on Testing Set
class_Logi_Reg = model_Logi_Reg.predict(norm_inp_test)

prob = model_Logi_Reg.predict_proba(norm_inp_test)
#plt.scatter(prob[:,0], predicted_class_Logi_Reg, s = 1)
plt.hist(prob[:, 0], bins = 10)

plt.xlabel("Probablities",fontsize=14)
plt.ylabel("Number of Data points",fontsize=14)
plt.title("Probablity Distribution for Logistic Regression", fontsize = 15)

plt.grid()

plt.show()
#plt.savefig("LogReg_prob_"+str(mod)+".png")

###Combining Input and Output of Test Data
final_result_logi_reg = np.column_stack((inp_val_test, class_Logi_Reg))

###Writing Output as a Dataframe 
df_logi_reg = pd.DataFrame(data=final_result_logi_reg, columns=train_set.columns.values)

###Plotting the Output
color=['blue','red']
plt.figure(figsize = (18,12))

plt.suptitle('BP-RP vs Gmag and pmRA vs pmDE Plots of Cluster Made Using Logistic Regression', fontsize = 18, fontweight='bold')
### Drawing Graph for Field Stars
dat0_LR = df_logi_reg[df_logi_reg["classes"]==0]
print("Number of Member Stars =",len(dat0_LR))
plt.subplot(2, 3, 1)
plt.scatter(dat0_LR["BP-RP"],dat0_LR["Gmag"],c=color[0], s = 1, label = "Member Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for Field Stars
dat1_LR = df_logi_reg[df_logi_reg["classes"]==1]
print("Number of Field Stars =",len(dat1_LR))
plt.subplot(2, 3, 2)
plt.scatter(dat1_LR["BP-RP"],dat1_LR["Gmag"],c=color[1], s = 1, label = "Field Star Predictions")

plt.title("Member Stars: "+str(len(dat0_LR))+"  Field Stars: "+str(len(dat1_LR))+"  Accuracy of Model: "+str(round(accuracy_Logi_Reg*100,2))+"  Balanced Accuracy of Model: "+str(round(bal_acc_Logi_Reg*100, 2))+"%"+"\nPrecision Score: "+str(round(prec_Logi_Reg, 4))+"  Recall Score: "+str(round(recall_Logi_Reg, 4))+"  AUC for ROC Curve: "+str(round(AUC_Logi_Reg, 4))+"  f1 Score of Model: "+str(round(f1_Logi_Reg,2))+"\nLogarithmic Loss of Model: "+str(round(log_loss_Logi_Reg,2))+"  Bias added: "+str(np.around(bias,4))+"  Iteration for Convergence: "+str(converg)+"  Confusion Matrix: "+str(conf_mat_LR), fontsize = 14)
plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

###Drawing The Combined Graph
plt.subplot(2, 3, 3)
plt.scatter(dat0_LR["BP-RP"],dat0_LR["Gmag"],c=color[0], s = 1, label = "Member Star Predictions")
plt.scatter(dat1_LR["BP-RP"],dat1_LR["Gmag"],c=color[1], s = 1, label = "Field Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 0 Stars
plt.subplot(2, 3, 4)
plt.scatter(dat0_LR["pmRA"], dat0_LR["pmDE"], c=color[0], s = 1, label = "class 0")

plt.xlim(-100,50)
plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 1 Stars

plt.subplot(2, 3, 5)
plt.scatter(dat1_LR["pmRA"], dat1_LR["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlim(-100,50)
plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Combined Graph for classified Stars
plt.subplot(2, 3, 6)
#plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat0_LR["pmRA"], dat0_LR["pmDE"], c=color[0], s = 1, label = "class 0")
plt.scatter(dat1_LR["pmRA"], dat1_LR["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlim(-100,50)
plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

plt.show()

#plt.savefig("LogReg_plots_"+str(mod)+".png")
#plt.savefig("Logi_Reg_.png")

"""## **Support Vector Machine**"""

###MODELLING VIA SUPPORT VECTOR MACHINE

mod = 11
model_SVM = svm.SVC(kernel='rbf', 
                    C = 1, 
                    degree=5, 
                    gamma= "scale", 
                    coef0=0.0, 
                    shrinking=True, 
                    probability=False, 
                    tol=0.001, 
                    cache_size=200, 
                    class_weight=None, 
                    verbose=1, 
                    max_iter=- 1, 
                    decision_function_shape='ovr', 
                    break_ties=False, 
                    random_state=None)
model_SVM.fit(xtrain,ytrain)

       
cls_wt = model_SVM.class_weight_
print("Model Class Weight:", cls_wt)

cls = model_SVM.classes_
print("Model Class:", cls)

'''
dual_coeff = model_SVM.dual_coef_
print("Model dual coefficient:", dual_coeff)
'''

fit_sta = model_SVM.fit_status_
print("Model fitting status:", fit_sta)

'''
sup = model_SVM.support_
print("Model Indices of support Vectors:", sup)
'''

sup_vec = model_SVM.support_vectors_
print("Model support Vectors shape:", sup_vec.shape)

n_sup = model_SVM.n_support_
print("Model Number of Support Vectors:", n_sup)
print( n_sup[0], n_sup[1])

dec_func = model_SVM.decision_function(xtrain)
print("Model decision function:", dec_func)

scr = model_SVM.score(xtest, ytest)
print("Model score:", round(scr*100,2),"%")


plt.scatter(sup_vec[:n_sup[0]-1,3], sup_vec[:n_sup[0]-1,4], color = "blue", s=1, label = "Member Stars")
plt.scatter(sup_vec[n_sup[0]:,3], sup_vec[n_sup[0]:,4], color = "red", s=1, label = "Field Stars")
plt.grid()
plt.title("Plot for Support Vectors in pmRA vs pmDE Plane\nMember Stars = "+ str(n_sup[0])+"  Field Stars = "+ str(n_sup[1]), fontsize = 14, fontweight='bold')
plt.xlabel("pmRA", fontsize = 15)
plt.ylabel("pmDE", fontsize = 15)
plt.legend()
plt.show()

#plt.savefig("SVM_dec_bou"+str(mod)+".png")


##Output for testing of Model
predicted_class_SVM = model_SVM.predict(xtest)

##Checking Model Accuracy
accuracy_SVM = accuracy_score(ytest,predicted_class_SVM)
print("The Accuracy for Support Vector Machine Model is",round(accuracy_SVM*100,2), "\n")

bal_acc_SVM = metrics.balanced_accuracy_score(ytest, predicted_class_SVM)
print("The Balanced Accuracy of Support Vector Machine Model is "+str(round(bal_acc_SVM*100,2))+"%.\n")

f1_SVM = metrics.f1_score(ytest, predicted_class_SVM)
print("The f1 Score of Support Vector Machine is "+str(round(f1_SVM,2))+".\n")

log_loss_SVM = metrics.log_loss(ytest, predicted_class_SVM)
print("The Log Loss of Support Vector Machine Model is "+str(round(log_loss_SVM,2))+".\n")

recall_svm = metrics.recall_score(ytest, predicted_class_SVM,  average='binary')
print("The Recall Score of Support Vector Machine Model is "+str(round(recall_svm,2))+".\n")

prec_SVM = metrics.precision_score(ytest, predicted_class_SVM)
print("The Precision Score of Support Vector Machine Model is "+str(round(prec_SVM,2))+".\n")

AUC_SVM = metrics.roc_auc_score(ytest, predicted_class_SVM)
print("The Value of AUC for Support Vector Machine Model is "+str(round(AUC_SVM,2))+".\n")

# Classification report
result1_SVM = classification_report(ytest, predicted_class_SVM)
print("\nClassification Report:")
print (result1_SVM)

conf_mat_SVM = confusion_matrix(ytest, predicted_class_SVM)
print("Confusion Matrix:")
print (conf_mat_SVM,"\n")

'''
##This gives Model Parameters
parameters_SVM = model_SVM.coef_
for i in range(len(total_set.columns.values)-1):
  print("Feature coefficients of", total_set.columns.values[i], "is given by :", parameters_SVM[0][i])
'''

yscore_SVM = model_SVM.decision_function(xtest)

plt.figure(figsize = (6,12))
plt.suptitle('ROC and Precision vs Recall Plots\n for different Gamma values for RBF Kernel', fontsize = 16, fontweight='bold')

#disp = metrics.plot_roc_curve(model_Logi_Reg, xtest, ytest) 
plt.subplot(2, 1, 1)
fpr_SVM, tpr_SVM, _ = metrics.roc_curve(ytest, yscore_SVM)
#fpr_SVM1, tpr_SVM1, _ = metrics.roc_curve(ytest, yscore_SVM1)
#fpr_SVM2, tpr_SVM2, _ = metrics.roc_curve(ytest, yscore_SVM2)


plt.plot(fpr_SVM, tpr_SVM, label = "ROC Curve for Gamma = Scale")
#plt.plot(fpr_SVM1, tpr_SVM1, label = "ROC Curve for Gamma = 10")
#plt.plot(fpr_SVM2, tpr_SVM2, label = "ROC Curve for Gamma = 100")

plt.plot([0,1], [0,1], label = "FPR = TPR Line", color = "black")

#plt.title("AUC for ROC Curve: "+str(AUC_Logi_Reg)+"\nPrecision Score: "+str(prec_Logi_Reg)+"\nRecall Score: "+str(recall_Logi_Reg), fontsize = 14)
plt.xlabel("False Positive Rate",fontsize=14)
plt.ylabel("True Positive Rate",fontsize=14)
plt.legend()
plt.grid()
plt.show()

#disp2 =metrics.plot_precision_recall_curve(model_Logi_Reg, xtest, ytest)
plt.subplot(2, 1, 2)
precision_SVM, recall_SVM, _ = metrics.precision_recall_curve(ytest, yscore_SVM)
#precision_SVM1, recall_SVM1, _ = metrics.precision_recall_curve(ytest, yscore_SVM1)
#precision_SVM2, recall_SVM2, _ = metrics.precision_recall_curve(ytest, yscore_SVM2)


plt.plot(recall_SVM, precision_SVM, label = "Recall-Precision Curve for Gamma = Scale")
#plt.plot(recall_SVM1, precision_SVM1, label = "Recall-Precision Curve for Gamma = 10")
#plt.plot(recall_SVM2, precision_SVM2, label = "Recall-Precision Curve for Gamma = 100")

plt.xlabel("Recall",fontsize=14)
plt.ylabel("Precision",fontsize=14)
plt.legend()
plt.grid()
plt.show()


#plt.savefig("SVM_curve_values_for_rbf_gam.png")
###Putting Logistic Regression on Testing Set
class_SVM = model_SVM.predict(norm_inp_test)

###Combining Input and Output of Test Data
final_result_SVM = np.column_stack((inp_val_test, class_SVM))

###Writing Output as a Dataframe 
df_SVM = pd.DataFrame(data=final_result_SVM, columns=train_set.columns.values)

###Plotting the Output
color=['blue','red']
plt.figure(figsize = (18,12))

plt.suptitle('BP-RP vs Gmag and pmRA vs pmDE Plots of Cluster Made Using Support Vector Machine', fontsize = 20, fontweight='bold')

dat0_SVM = df_SVM[df_SVM["classes"]==0]
print("Number of Field Stars =",len(dat0_SVM))
plt.subplot(2, 3, 1)
plt.scatter(dat0_SVM["BP-RP"],dat0_SVM["Gmag"],c=color[0], s = 1, label = "Field Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

dat1_SVM = df_SVM[df_SVM["classes"]==1]
print("Number of Member Stars =",len(dat1_SVM))
plt.subplot(2, 3, 2)
plt.scatter(dat1_SVM["BP-RP"],dat1_SVM["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")

plt.title("Member Stars: "+str(len(dat0_SVM))+"  Field Stars: "+str(len(dat1_SVM))+"  Accuracy of Model: "+str(round(accuracy_SVM*100,2))+
          "  Balanced Accuracy of Model: "+str(round(bal_acc_SVM*100, 2))+"%"+"\nPrecision Score: "+str(round(prec_SVM, 4))+
          "  AUC for ROC Curve: "+str(round(AUC_SVM, 4))+"  f1 Score of Model: "+str(round(f1_SVM,2))+
          "  Recall Score: "+str(round(recall_svm, 4))+"\nLogarithmic Loss of Model: "+str(round(log_loss_SVM,4))+"  Number of Support Vectors: "+str(n_sup)+"  Confusion Matrix: "+str(conf_mat_SVM), fontsize = 14)
plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

###Drawing The Combined Graph
plt.subplot(2, 3, 3)
plt.scatter(dat1_SVM["BP-RP"],dat1_SVM["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")
plt.scatter(dat0_SVM["BP-RP"],dat0_SVM["Gmag"],c=color[0], s = 1, label = "Field Star Predictions")
#plt.scatter(dat1_SVM["BP-RP"],dat1_SVM["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 0 Stars
plt.subplot(2, 3, 4)
plt.scatter(dat0_SVM["pmRA"], dat0_SVM["pmDE"], c=color[0], s = 1, label = "class 0")

plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 1 Stars

plt.subplot(2, 3, 5)
plt.scatter(dat1_SVM["pmRA"], dat1_SVM["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlim(-100,50)
plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Combined Graph for classified Stars
plt.subplot(2, 3, 6)
#plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat1_SVM["pmRA"], dat1_SVM["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat0_SVM["pmRA"], dat0_SVM["pmDE"], c=color[0], s = 1, label = "class 0")
#plt.scatter(dat1_SVM["pmRA"], dat1_SVM["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlim(-100,50)
plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

plt.show()
#plt.savefig("SVM_"+str(mod)+".png")

"""## **Random Forest**"""

###MODELLING VIA SUPPORT VECTOR MACHINE
mod = 12

model_RF = RandomForestClassifier(n_estimators = 100, 
                                  criterion='gini', 
                                  max_depth=None, 
                                  min_samples_split=2, 
                                  min_samples_leaf=8, 
                                  min_weight_fraction_leaf=0.0, 
                                  max_features="sqrt", 
                                  max_leaf_nodes=None, 
                                  min_impurity_decrease=0.0, 
                                  min_impurity_split=None, 
                                  bootstrap=True, 
                                  oob_score=False, 
                                  n_jobs=None, 
                                  random_state=None, 
                                  verbose=0, 
                                  warm_start=False, 
                                  class_weight=None, 
                                  ccp_alpha=0.0, 
                                  max_samples=None)
model_RF.fit(xtrain,ytrain)


'''
bas_E = model_RF.base_estimator_
print("Base estimators for this model are", bas_E)

est = model_RF.estimators_
print("Estimators for this model are", est)
'''

cls = model_RF.classes_
print("Class labels for this model are", cls)

n_cls = model_RF.n_classes_
print("Number of classes in this model are", n_cls)

n_feat = model_RF.n_features_
print("Number of features in this model are", n_feat)

n_out = model_RF.n_outputs_
print("Number of output in this model are", n_out)

f_imp = model_RF.feature_importances_
print("Feature importances of this model are", n_out)

'''
path = model_RF.decision_path(xtrain)
print("Decision path of forest are", path)

prob = model_RF.predict_proba
print("the probablities are", prob)
'''

##Output for testing of Model
predicted_class_RF = model_RF.predict(xtest)

##Checking Model Accuracy
accuracy_RF = accuracy_score(ytest,predicted_class_RF)
print("The Accuracy for Random Forest is",round(accuracy_RF*100,2))

bal_acc_RF = metrics.balanced_accuracy_score(ytest, predicted_class_RF)
print("\nThe Balanced Accuracy of Random Forest Model is "+str(round(bal_acc_RF*100,2))+"%.\n")

f1_RF = metrics.f1_score(ytest, predicted_class_RF)
print("The f1 Score of Random Forest is "+str(round(f1_RF,2))+".\n")

log_loss_RF = metrics.log_loss(ytest, predicted_class_RF)
print("The Log Loss of Random Forest Model is "+str(round(log_loss_RF,2))+".\n")

recall_rf = metrics.recall_score(ytest, predicted_class_RF,  average='binary')
print("The Recall Score of Random Forest Model is "+str(round(recall_rf,2))+".\n")

prec_RF = metrics.precision_score(ytest, predicted_class_RF)
print("The Precision Score of Random Forest Model is "+str(round(prec_RF,2))+".\n")

AUC_RF = metrics.roc_auc_score(ytest, predicted_class_RF)
print("The Value of AUC for Random Forest Model is "+str(round(AUC_RF,2))+".\n")

# Classification report
result1_RF = classification_report(ytest, predicted_class_RF)
print("\nClassification Report:")
print (result1_RF)

conf_mat_RF = confusion_matrix(ytest, predicted_class_RF)
print("Confusion Matrix:")
print (conf_mat_RF,"\n")

ax = plt.gca()

#roc1 = metrics.plot_precision_recall_curve(model_RF1, xtest, ytest, ax=ax, alpha=0.8, label = "min sample leaf = 1")
roc = metrics.plot_precision_recall_curve(model_RF, xtest, ytest, ax=ax, alpha=0.8, label = "min sample leaf = 4")
#roc2 = metrics.plot_precision_recall_curve(model_RF2, xtest, ytest, ax=ax, alpha=0.8, label = "min sample leaf = 8")

plt.xlabel("Recall",fontsize=14)
plt.ylabel("Precision",fontsize=14)
ax.set_title('Recall-Precision Curve for Random Forest\nfor various min sample leaf', fontsize = 16, fontweight='bold')
plt.legend()
plt.grid()
plt.show()

#plt.savefig("RF_Recall_prec_curve_for_leaf.png")


###Putting Logistic Regression on Testing Set
class_RF = model_RF.predict(norm_inp_test)

ax = plt.gca()

roc = metrics.plot_roc_curve(model_RF, xtest, ytest, ax=ax, alpha=0.8, label = "min sample leaf = 1")
#roc1 = metrics.plot_roc_curve(model_RF1, xtest, ytest, ax=ax, alpha=0.8, label = "min sample leaf = 4")
#roc2 = metrics.plot_roc_curve(model_RF2, xtest, ytest, ax=ax, alpha=0.8, label = "min sample leaf = 8")

plt.xlabel("False Positive Rate",fontsize=14)
plt.ylabel("True Positive Rate",fontsize=14)
ax.set_title('ROC Curve for Random Forest\nfor various min sample leaf', fontsize = 16, fontweight='bold')
plt.legend()
plt.grid()
plt.show()

#plt.savefig("RF_ROC_for_leaf.png")

###Combining Input and Output of Test Data
final_result_RF = np.column_stack((inp_val_test, class_RF))

###Writing Output as a Dataframe 
df_RF = pd.DataFrame(data=final_result_RF, columns=train_set.columns.values)

###Plotting the Output
color=['blue','red']
plt.figure(figsize = (18,12))

plt.suptitle('BP-RP vs Gmag and pmRA vs pmDE Plots of Cluster Made Using Random Forest', fontsize = 20)

dat0_RF = df_RF[df_RF["classes"]==0]
print("Number of Field Stars =",len(dat0_RF))
plt.subplot(2, 3, 1)
plt.scatter(dat0_RF["BP-RP"],dat0_RF["Gmag"],c=color[0], s = 1, label = "Field Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

dat1_RF = df_RF[df_RF["classes"]==1]
print("Number of Member Stars =",len(dat1_RF))
plt.subplot(2, 3, 2)
plt.scatter(dat1_RF["BP-RP"],dat1_RF["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")

plt.title("Member Stars: "+str(len(dat0_RF))+"  Field Stars: "+str(len(dat1_RF))+"  Accuracy of Model: "+str(round(accuracy_RF*100,2))+
          "  Balanced Accuracy of Model: "+str(round(bal_acc_RF*100, 2))+"%"+"\nPrecision Score: "+str(round(prec_RF, 4))+
          "  AUC for ROC Curve: "+str(round(AUC_RF, 4))+"  f1 Score of Model: "+str(round(f1_RF,2))+
          "  Recall Score: "+str(round(recall_rf, 4))+"\nLogarithmic Loss of Model: "+str(round(log_loss_RF,4))+"  Confusion Matrix: "+str(conf_mat_RF), fontsize = 14)


plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

###Drawing The Combined Graph
plt.subplot(2, 3, 3)
plt.scatter(dat1_RF["BP-RP"],dat1_RF["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")
plt.scatter(dat0_RF["BP-RP"],dat0_RF["Gmag"],c=color[0], s = 1, label = "Field Star Predictions")
#plt.scatter(dat1_RF["BP-RP"],dat1_RF["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 0 Stars
plt.subplot(2, 3, 4)
plt.scatter(dat0_RF["pmRA"], dat0_RF["pmDE"], c=color[0], s = 1, label = "class 0")


plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 1 Stars

plt.subplot(2, 3, 5)
plt.scatter(dat1_RF["pmRA"], dat1_RF["pmDE"], c=color[1], s = 1, label = "Class 1")

#plt.xlim(-100,50)
#plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Combined Graph for classified Stars
plt.subplot(2, 3, 6)
#plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat1_RF["pmRA"], dat1_RF["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat0_RF["pmRA"], dat0_RF["pmDE"], c=color[0], s = 1, label = "class 0")
#plt.scatter(dat1_RF["pmRA"], dat1_RF["pmDE"], c=color[1], s = 1, label = "Class 1")

plt.xlim(-150,100)
plt.ylim(-200,100)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

plt.show()
#plt.savefig("RF_"+str(mod)+".png")

"""## **Artificial Neural Network**"""

###Training The Model
mod = 14

model_ANN = MLPClassifier(hidden_layer_sizes=(2,4,2),
                          activation='relu',
                          solver='adam',
                          max_iter=200,
                          alpha=0.0001,
                          batch_size='auto',
                          learning_rate='constant',
                          learning_rate_init=0.001,
                          power_t=0.5,
                          shuffle=False,
                          random_state=None,
                          tol=0.0001,
                          verbose=False,
                          warm_start=False,
                          momentum=0.9,
                          nesterovs_momentum=True,
                          early_stopping=False,
                          validation_fraction=0.1,
                          beta_1=0.9,
                          beta_2=0.999,
                          epsilon=1e-08,
                          n_iter_no_change=10,
                          max_fun=15000)  
model_ANN.fit(xtrain,ytrain)


model_ANN1 = MLPClassifier(hidden_layer_sizes=(2,4,2),
                          activation='relu',
                          solver='adam',
                          max_iter=200,
                          alpha=0.0001,
                          batch_size='auto',
                          learning_rate='constant',
                          learning_rate_init=0.001,
                          power_t=0.5,
                          shuffle=False,
                          random_state=None,
                          tol=0.0001,
                          verbose=True,
                          warm_start=False,
                          momentum=0.9,
                          nesterovs_momentum=True,
                          early_stopping=False,
                          validation_fraction=0.1,
                          beta_1=0.9,
                          beta_2=0.999,
                          epsilon=1e-08,
                          n_iter_no_change=10,
                          max_fun=15000)  
model_ANN1.fit(xtrain,ytrain)


model_ANN2 = MLPClassifier(hidden_layer_sizes=(2,4,2),
                          activation='relu',
                          solver='adam',
                          max_iter=200,
                          alpha=0.000001,
                          batch_size='auto',
                          learning_rate='constant',
                          learning_rate_init=0.001,
                          power_t=0.5,
                          shuffle=True,
                          random_state=None,
                          tol=0.0001,
                          verbose=True,
                          warm_start=False,
                          momentum=0.9,
                          nesterovs_momentum=True,
                          early_stopping=False,
                          validation_fraction=0.1,
                          beta_1=0.9,
                          beta_2=0.999,
                          epsilon=1e-08,
                          n_iter_no_change=10,
                          max_fun=15000)  
model_ANN2.fit(xtrain,ytrain)


###Testing the Model
predict_class_prob_ANN = model_ANN.predict_proba(xtest)
predicted_class_ANN = model_ANN.predict(xtest)
predicted_class_ANN1 = model_ANN1.predict(xtest)
predicted_class_ANN2 = model_ANN2.predict(xtest)


clss_ANN = model_ANN.classes_
print("Classes for the model are:", clss_ANN)

loss_ANN = model_ANN.loss_
print("losses for the model are:", loss_ANN)

best_loss_ANN = model_ANN.best_loss_
print("best loss for the model is:", best_loss_ANN)


loss_curve_ANN = model_ANN.loss_curve_
print("Loss Curve for the model are:", loss_curve_ANN)
x = np.linspace(1, len(loss_curve_ANN), len(loss_curve_ANN))


plt.scatter(x, loss_curve_ANN, s = 6, color = "blue", label = "Loss")
plt.plot(x, loss_curve_ANN, color = "red", label = "trendline")
plt.xlabel("ITERATIONS",fontsize=14)
plt.ylabel("LOSS VALUE",fontsize=14)
plt.title('LOSS-CURVE\nFinal Loss = '+str(round(loss_ANN, 6)), fontsize = 16, fontweight='bold')
plt.legend()
plt.grid()
plt.show()

#plt.savefig("ANN_Loss_curve"+str(mod)+".png")


t_ANN = model_ANN.t_
print("Number of training samples for the model are:", t_ANN)

'''
coff_ANN = model_ANN.coefs_
print("Weight for the layers for the model are:", coff_ANN.shape)

intcpt_ANN = model_ANN.intercepts_
print("Bias for for the layers for the model are:", intcpt_ANN.shape)
'''

iter_ANN = model_ANN.n_iter_
print("Number of iterations for the model are:", iter_ANN)

lay_ANN = model_ANN.n_layers_
print("Number of layers for the model are:", lay_ANN)

out_ANN = model_ANN.n_outputs_
print("Number of outputs for the model are:", out_ANN)

acti_ANN = model_ANN.out_activation_
print("output activation function for the model are:", acti_ANN)

##Checking Model Accuracy
accuracy_ANN = accuracy_score(ytest,predicted_class_ANN)
print("The Accuracy for Artificial Neural Network Model is",round(accuracy_ANN*100,2))

bal_acc_ANN = metrics.balanced_accuracy_score(ytest, predicted_class_ANN)
print("\nThe Balanced Accuracy of Artificial Neural Network Model is "+str(round(bal_acc_ANN*100,2))+"%.\n")

f1_ANN = metrics.f1_score(ytest, predicted_class_ANN)
print("The f1 Score of Artificial Neural Network is "+str(round(f1_ANN,2))+".\n")

log_loss_ANN = metrics.log_loss(ytest, predicted_class_ANN)
print("The Log Loss of Artificial Neural Network Model is "+str(round(log_loss_ANN,2))+".\n")

recall_ann = metrics.recall_score(ytest, predicted_class_ANN,  average='binary')
print("The Recall Score of Artificial Neural Network Model is "+str(round(recall_ann,2))+".\n")

prec_ANN = metrics.precision_score(ytest, predicted_class_ANN)
print("The Precision Score of Artificial Neural Network Model is "+str(round(prec_ANN,2))+".\n")

AUC_ANN = metrics.roc_auc_score(ytest, predicted_class_ANN)
print("The Value of AUC for Artificial Neural Network Model is "+str(round(AUC_ANN,2))+".\n")


# Classification report
result1_ANN = classification_report(ytest, predicted_class_ANN)
print("\nClassification Report:")
print (result1_ANN)

conf_mat_ANN = confusion_matrix(ytest, predicted_class_ANN)
print("Confusion Matrix:")
print (conf_mat_ANN)

###Putting Logistic Regression on Testing Set
class_ANN = model_ANN.predict(norm_inp_test)
class_ANN1 = model_ANN1.predict(norm_inp_test)
class_ANN2 = model_ANN2.predict(norm_inp_test)

ax = plt.gca()

roc_ANN = metrics.plot_roc_curve(model_ANN, xtest, ytest, ax=ax, alpha=0.8, label = "shuffle = True")
roc1_ANN1 = metrics.plot_roc_curve(model_ANN1, xtest, ytest, ax=ax, alpha=0.8, label = "shuffle = False")
roc2_ANN2 = metrics.plot_roc_curve(model_ANN2, xtest, ytest, ax=ax, alpha=0.8, label = "alpha = 0.000001")
#roc2_ANN3 = metrics.plot_roc_curve(model_ANN3, xtest, ytest, ax=ax, alpha=0.8, label = "activation function = logistic")

plt.xlabel("False Positive Rate",fontsize=14)
plt.ylabel("True Positive Rate",fontsize=14)
ax.set_title('ROC Curve for ANN\nfor various Shuffle values', fontsize = 16, fontweight='bold')
plt.legend()
plt.grid()
plt.show()

#plt.savefig("ANN_ROC_for_shf.png")

ax = plt.gca()

rec_ANN = metrics.plot_precision_recall_curve(model_ANN, xtest, ytest, ax=ax, alpha=0.8, label = "shuffle = True")
rec_ANN1 = metrics.plot_precision_recall_curve(model_ANN1, xtest, ytest, ax=ax, alpha=0.8, label = "Shuffle = False")
rec_ANN2 = metrics.plot_precision_recall_curve(model_ANN2, xtest, ytest, ax=ax, alpha=0.8, label = "alpha = 0.000001")
#rec_ANN3 = metrics.plot_precision_recall_curve(model_ANN3, xtest, ytest, ax=ax, alpha=0.8, label = "activation function = logistic")

plt.xlabel("Recall",fontsize=14)
plt.ylabel("Precision",fontsize=14)
ax.set_title('Recall-Precision Curve for ANN\nfor various Shuffle Values', fontsize = 16, fontweight='bold')
plt.legend()
plt.grid()
plt.show()

#plt.savefig("ANN_RP_curve_for_shf.png")

###Combining Input and Output of Test Data
final_result_ANN = np.column_stack((inp_val_test, class_ANN))

###Writing Output as a Dataframe 
df_ANN = pd.DataFrame(data=final_result_ANN, columns=total_set.columns.values)

###Plotting the Output
color=['blue','red']
plt.figure(figsize = (18,12))

plt.suptitle('BP-RP vs Gmag and pmRA vs pmDE Plots of Cluster Made Using Artificial Neural Network', fontsize = 20)

dat0_ANN = df_ANN[df_ANN["classes"]==0]
print("Number of Field Stars =",len(dat0_ANN))
plt.subplot(2, 3, 1)
plt.scatter(dat0_ANN["BP-RP"],dat0_ANN["Gmag"],c=color[0], s = 1, label = "Field Star Predictions")

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

dat1_ANN = df_ANN[df_ANN["classes"]==1]
print("Number of Member Stars =",len(dat1_ANN))
plt.subplot(2, 3, 2)
plt.scatter(dat1_ANN["BP-RP"],dat1_ANN["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")

plt.title("Member Stars: "+str(len(dat0_ANN))+"  Field Stars: "+str(len(dat1_ANN))+"  Accuracy of Model: "+str(round(accuracy_ANN*100,2))+
          "  Balanced Accuracy of Model: "+str(round(bal_acc_ANN*100, 2))+"%"+"\nPrecision Score: "+str(round(prec_ANN, 4))+
          "  AUC for ROC Curve: "+str(round(AUC_ANN, 4))+"  f1 Score of Model: "+str(round(f1_ANN,2))+
          "  Recall Score: "+str(round(recall_ann, 4))+"\n Final Loss = "+str(loss_ANN)+"  Logarithmic Loss of Model: "+str(round(log_loss_ANN,4))+"  Confusion Matrix: "+str(conf_mat_ANN), fontsize = 14)

plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

###Drawing The Combined Graph
plt.subplot(2, 3, 3)
plt.scatter(dat1_ANN["BP-RP"],dat1_ANN["Gmag"],c=color[1], s = 1, label = "Member Star Predictions")
plt.scatter(dat0_ANN["BP-RP"],dat0_ANN["Gmag"],c=color[0], s = 1, label = "Field Star Predictions")


plt.ylim(22,7)
plt.xlabel("BP-RP",fontsize=14)
plt.ylabel("G_Mag",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 0 Stars
plt.subplot(2, 3, 4)
plt.scatter(dat0_ANN["pmRA"], dat0_ANN["pmDE"], c=color[0], s = 1, label = "class 0")


plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Graph for class 1 Stars

plt.subplot(2, 3, 5)
plt.scatter(dat1_ANN["pmRA"], dat1_ANN["pmDE"], c=color[1], s = 1, label = "Class 1")

#plt.xlim(-100,50)
#plt.ylim(-100,50)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

### Drawing Combined Graph for classified Stars
plt.subplot(2, 3, 6)
#plt.scatter(cls1["pmRA"], cls1["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat1_ANN["pmRA"], dat1_ANN["pmDE"], c=color[1], s = 1, label = "Class 1")
plt.scatter(dat0_ANN["pmRA"], dat0_ANN["pmDE"], c=color[0], s = 1, label = "class 0")


plt.xlim(-150,100)
plt.ylim(-200,100)
plt.xlabel("pmRA",fontsize=14)
plt.ylabel("pmDE",fontsize=14)
plt.legend()
plt.grid()

plt.show()
#plt.savefig("ANN_"+str(mod)+".png")

